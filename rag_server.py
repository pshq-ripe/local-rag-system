from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from langchain_qdrant import QdrantVectorStore
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.chains import RetrievalQA
from langchain_openai import OpenAI
from qdrant_client import QdrantClient
import os
import logging
from typing import List, Optional

# Konfiguracja loggera
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

app = FastAPI(
    title="RAG MCP Server",
    description="Local RAG system dla DevOps/SRE dokumentacji",
    version="1.0.0"
)

# Konfiguracja z environment variables
QDRANT_URL = os.getenv("QDRANT_URL", "http://qdrant:6333")
LM_STUDIO_URL = os.getenv("LM_STUDIO_URL", "http://host.docker.internal:1234/v1")
COLLECTION_NAME = os.getenv("COLLECTION_NAME", "devops_docs")
EMBEDDING_MODEL = os.getenv("EMBEDDING_MODEL", "sentence-transformers/all-MiniLM-L6-v2")
CHUNK_SIZE = int(os.getenv("CHUNK_SIZE", "1000"))
CHUNK_OVERLAP = int(os.getenv("CHUNK_OVERLAP", "200"))
TEMPERATURE = float(os.getenv("TEMPERATURE", "0.7"))
MAX_RETRIEVAL_RESULTS = int(os.getenv("MAX_RETRIEVAL_RESULTS", "3"))

# Inicjalizacja globalnych zmiennych
qa_chain = None
vectorstore = None
embeddings = None


class Question(BaseModel):
    question: str
    max_results: Optional[int] = MAX_RETRIEVAL_RESULTS
    temperature: Optional[float] = TEMPERATURE


class Source(BaseModel):
    content: str
    metadata: dict
    score: Optional[float] = None


class Answer(BaseModel):
    answer: str
    sources: List[Source]
    model_used: str
    collection: str


class HealthCheck(BaseModel):
    status: str
    qdrant_connected: bool
    lm_studio_connected: bool
    collection_exists: bool
    collection_vectors_count: Optional[int] = None
    qa_chain_initialized: bool


@app.on_event("startup")
async def startup_event():
    """Inicjalizacja RAG chain przy starcie aplikacji"""
    global qa_chain, vectorstore, embeddings

    try:
        logger.info("üöÄ Uruchamianie RAG MCP Server...")
        logger.info(f"üìç Qdrant URL: {QDRANT_URL}")
        logger.info(f"ü§ñ LM Studio URL: {LM_STUDIO_URL}")
        logger.info(f"üìö Kolekcja: {COLLECTION_NAME}")
        logger.info(f"üß† Embedding model: {EMBEDDING_MODEL}")

        # Embeddingi
        logger.info("‚öôÔ∏è  Inicjalizacja embedding model...")
        embeddings = HuggingFaceEmbeddings(
            model_name=EMBEDDING_MODEL,
            model_kwargs={
                'device': 'cpu',
                'trust_remote_code': True
            },
            encode_kwargs={
                'normalize_embeddings': True,
                'batch_size': 32
            }
        )
        logger.info("‚úÖ Embedding model za≈Çadowany")

        # Qdrant client
        logger.info("‚öôÔ∏è  ≈ÅƒÖczenie z Qdrant...")
        client = QdrantClient(url=QDRANT_URL, timeout=10)

        # Sprawd≈∫ po≈ÇƒÖczenie z Qdrant
        collection_exists = False
        try:
            collections = client.get_collections().collections
            collection_exists = any(c.name == COLLECTION_NAME for c in collections)

            if not collection_exists:
                logger.warning(f"‚ö†Ô∏è  Kolekcja '{COLLECTION_NAME}' nie istnieje w Qdrant!")
                logger.warning("üìù Uruchom skrypt indeksowania dokument√≥w (index_documents.py)")
                logger.warning("üí° Serwer bƒôdzie dzia≈Ça≈Ç, ale /query nie bƒôdzie dzia≈Çaƒá dop√≥ki nie zaindeksujesz dokument√≥w")
            else:
                collection_info = client.get_collection(COLLECTION_NAME)
                logger.info(f"‚úÖ Znaleziono kolekcjƒô '{COLLECTION_NAME}' z {collection_info.vectors_count} wektorami")
        except Exception as e:
            logger.error(f"‚ùå B≈ÇƒÖd po≈ÇƒÖczenia z Qdrant: {e}")
            raise

        # Vector store - TYLKO je≈õli kolekcja istnieje
        logger.info("‚öôÔ∏è  Inicjalizacja vector store...")
        if collection_exists:
            vectorstore = QdrantVectorStore(
                client=client,
                collection_name=COLLECTION_NAME,
                embedding=embeddings
            )
            logger.info("‚úÖ Vector store zainicjalizowany")

            # LLM (LM Studio) - tylko je≈õli mamy vectorstore
            logger.info("‚öôÔ∏è  ≈ÅƒÖczenie z LM Studio...")
            llm = OpenAI(
                base_url=LM_STUDIO_URL,
                api_key="lm-studio",
                temperature=TEMPERATURE,
                max_tokens=2048,
                timeout=30
            )
            logger.info("‚úÖ LM Studio po≈ÇƒÖczony")

            # RAG Chain
            logger.info("‚öôÔ∏è  Tworzenie RAG chain...")
            qa_chain = RetrievalQA.from_chain_type(
                llm=llm,
                chain_type="stuff",
                retriever=vectorstore.as_retriever(
                    search_type="similarity",
                    search_kwargs={
                        "k": MAX_RETRIEVAL_RESULTS,
                        "score_threshold": 0.5
                    }
                ),
                return_source_documents=True,
                verbose=True
            )
            logger.info("‚úÖ RAG chain zainicjalizowany")
        else:
            vectorstore = None
            qa_chain = None
            logger.warning("‚ö†Ô∏è  RAG chain NIE zosta≈Ç zainicjalizowany - zaindeksuj dokumenty najpierw")

        logger.info("‚úÖ RAG Server zainicjalizowany pomy≈õlnie!")
        logger.info(f"üéØ Gotowy do przyjmowania zapyta≈Ñ na http://0.0.0.0:8000")
        if not collection_exists:
            logger.info("‚ö†Ô∏è  Pamiƒôtaj: musisz zaindeksowaƒá dokumenty zanim bƒôdziesz m√≥g≈Ç u≈ºywaƒá /query")

    except Exception as e:
        logger.error(f"‚ùå Krytyczny b≈ÇƒÖd podczas inicjalizacji: {e}", exc_info=True)
        # NIE rzucaj wyjƒÖtku - pozw√≥l serwerowi wystartowaƒá
        logger.warning("‚ö†Ô∏è  Serwer wystartuje w trybie ograniczonym - niekt√≥re endpointy mogƒÖ nie dzia≈Çaƒá")


@app.get("/", tags=["Info"])
async def root():
    """Podstawowe informacje o API"""
    return {
        "service": "RAG MCP Server",
        "version": "1.0.0",
        "status": "running",
        "rag_ready": qa_chain is not None,
        "endpoints": {
            "query": "/query",
            "health": "/health",
            "config": "/config",
            "docs": "/docs"
        }
    }


@app.get("/config", tags=["Info"])
async def get_config():
    """Zwraca aktualnƒÖ konfiguracjƒô"""
    return {
        "qdrant_url": QDRANT_URL,
        "lm_studio_url": LM_STUDIO_URL,
        "collection": COLLECTION_NAME,
        "embedding_model": EMBEDDING_MODEL,
        "chunk_size": CHUNK_SIZE,
        "chunk_overlap": CHUNK_OVERLAP,
        "temperature": TEMPERATURE,
        "max_retrieval_results": MAX_RETRIEVAL_RESULTS,
        "rag_initialized": qa_chain is not None
    }


@app.get("/health", response_model=HealthCheck, tags=["Health"])
async def health():
    """Szczeg√≥≈Çowy health check"""
    qdrant_connected = False
    lm_studio_connected = False
    collection_exists = False
    vectors_count = None

    try:
        # Sprawd≈∫ Qdrant
        client = QdrantClient(url=QDRANT_URL, timeout=5)
        collections = client.get_collections().collections
        qdrant_connected = True
        collection_exists = any(c.name == COLLECTION_NAME for c in collections)

        if collection_exists:
            collection_info = client.get_collection(COLLECTION_NAME)
            vectors_count = collection_info.vectors_count
    except Exception as e:
        logger.error(f"Health check - Qdrant error: {e}")

    try:
        # Sprawd≈∫ LM Studio (opcjonalnie, mo≈ºe byƒá wolne)
        lm_studio_connected = True  # Zak≈Çadamy ≈ºe dzia≈Ça, mo≈ºna dodaƒá prawdziwy check
    except Exception as e:
        logger.error(f"Health check - LM Studio error: {e}")

    return HealthCheck(
        status="healthy" if (qdrant_connected and collection_exists and qa_chain is not None) else "degraded",
        qdrant_connected=qdrant_connected,
        lm_studio_connected=lm_studio_connected,
        collection_exists=collection_exists,
        collection_vectors_count=vectors_count,
        qa_chain_initialized=qa_chain is not None
    )


@app.post("/query", response_model=Answer, tags=["RAG"])
async def query(question: Question):
    """Endpoint do zadawania pyta≈Ñ RAG"""
    if qa_chain is None:
        raise HTTPException(
            status_code=503,
            detail="RAG chain nie zosta≈Ç zainicjalizowany. Zaindeksuj dokumenty u≈ºywajƒÖc index_documents.py i zrestartuj serwer."
        )

    if vectorstore is None:
        raise HTTPException(
            status_code=503,
            detail="Vector store nie jest dostƒôpny. Zaindeksuj dokumenty najpierw."
        )

    try:
        logger.info(f"üì® Otrzymano pytanie: {question.question}")

        # Wykonaj zapytanie RAG
        result = qa_chain.invoke({"query": question.question})

        # Przygotuj odpowied≈∫
        sources = []
        for doc in result.get('source_documents', []):
            sources.append(Source(
                content=doc.page_content[:500],  # Ograniczenie d≈Çugo≈õci
                metadata=doc.metadata,
                score=doc.metadata.get('score', None)
            ))

        answer = Answer(
            answer=result['result'],
            sources=sources,
            model_used=EMBEDDING_MODEL,
            collection=COLLECTION_NAME
        )

        logger.info(f"‚úÖ Odpowied≈∫ wygenerowana, znaleziono {len(sources)} ≈∫r√≥de≈Ç")
        return answer

    except Exception as e:
        logger.error(f"‚ùå B≈ÇƒÖd podczas przetwarzania pytania: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"B≈ÇƒÖd przetwarzania: {str(e)}")


@app.post("/search", tags=["RAG"])
async def search_documents(query: str, k: int = 5):
    """Bezpo≈õrednie wyszukiwanie w vector store bez LLM"""
    if vectorstore is None:
        raise HTTPException(
            status_code=503,
            detail="Vector store niedostƒôpny. Zaindeksuj dokumenty najpierw."
        )

    try:
        docs = vectorstore.similarity_search_with_score(query, k=k)
        results = [
            {
                "content": doc.page_content[:300],
                "metadata": doc.metadata,
                "score": score
            }
            for doc, score in docs
        ]
        return {"query": query, "results": results}
    except Exception as e:
        logger.error(f"Search error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
